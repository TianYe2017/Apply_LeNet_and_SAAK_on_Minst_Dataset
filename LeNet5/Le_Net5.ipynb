{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import os.path as osp\n",
    "import os\n",
    "import subprocess\n",
    "import tensorflow as tf\n",
    "import tensorflow.contrib.slim as slim\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#sess = tf.Session()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def loading_mnist():\n",
    "    data_url = 'http://yann.lecun.com/exdb/mnist/'\n",
    "    download_dir = osp.join('mnist_data','mnist')\n",
    "    if osp.exists(download_dir):\n",
    "        print('minist is found, no need to download')\n",
    "        return\n",
    "    else:\n",
    "        os.mkdir(download_dir)\n",
    "        \n",
    "    keys = ['train-images-idx3-ubyte.gz', 't10k-images-idx3-ubyte.gz',\n",
    "            'train-labels-idx1-ubyte.gz', 't10k-labels-idx1-ubyte.gz']\n",
    "    for k in keys:\n",
    "        url = (data_url+k).format(**locals())\n",
    "        target_path = osp.join(download_dir, k)\n",
    "        cmd = ['curl', url, '-o', target_path]\n",
    "        print('Downloading ', k)\n",
    "        subprocess.call(cmd)\n",
    "        cmd = ['gzip', '-d', target_path]\n",
    "        print('Unzip ', k)\n",
    "        subprocess.call(cmd)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "minist is found, no need to download\n"
     ]
    }
   ],
   "source": [
    "loading_mnist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def read_mnist():\n",
    "    num_train = 60000\n",
    "    num_test = 10000    \n",
    "    def read(filename,num,shape):\n",
    "        fd = open(osp.join('mnist_data\\\\mnist', filename))\n",
    "        loaded = np.fromfile(file=fd, dtype=np.uint8)\n",
    "        return loaded[num:].reshape(shape).astype(np.float)\n",
    "    \n",
    "    train_image = read('train-images-idx3-ubyte', 16, (num_train, 28, 28, 1))\n",
    "    train_label = read('train-labels-idx1-ubyte', 8, num_train)\n",
    "    test_image = read('t10k-images-idx3-ubyte', 16, (num_test, 28, 28, 1))\n",
    "    test_label = read('t10k-labels-idx1-ubyte', 8, num_test)\n",
    "    return train_image, train_label, test_image, test_label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "60000\n",
      "60000\n",
      "10000\n",
      "10000\n"
     ]
    }
   ],
   "source": [
    "X_train, Y_train, X_test, Y_test = read_mnist()\n",
    "print(len(X_train))\n",
    "print(len(Y_train))\n",
    "print(len(X_test))\n",
    "print(len(Y_test))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def conv2d_from_contrib(input, filter_size, num_stride, num_filter):\n",
    "    return tf.contrib.layers.conv2d(input, num_outputs=num_filter, kernel_size=filter_size, stride=num_stride,\n",
    "                                   activation_fn = tf.nn.relu, weights_initializer=tf.contrib.layers.xavier_initializer())\n",
    "\n",
    "def fully_connected(input, output_dim, weight_init = tf.random_normal_initializer(0.0, 0.02)):\n",
    "    return tf.contrib.layers.fully_connected(input, output_dim, activation_fn=None, weights_initializer=weight_init)\n",
    "\n",
    "def max_pool(input, kernel_size, stride):\n",
    "    ksize = [1, kernel_size, kernel_size, 1]\n",
    "    strides = [1, stride, stride, 1]\n",
    "    return tf.nn.max_pool(input, ksize=ksize, strides=strides, padding='SAME')\n",
    "\n",
    "def dropout(input,rate_p):\n",
    "    return tf.layers.dropout(input, rate=rate_p)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "class Le_Net5(object):\n",
    "    def __init__(self):\n",
    "        self.num_epoch = 10\n",
    "        self.batch_size = 64\n",
    "        self.log_step = 50\n",
    "        self.highest_accuracy=0.\n",
    "        self._build_model()\n",
    "    \n",
    "    def _model(self):\n",
    "        print('---Le_Net5---')\n",
    "        \n",
    "        print('intput: ' + str(self.X.get_shape()))\n",
    "        \n",
    "        with tf.variable_scope('conv1'):\n",
    "            self.conv1 = conv2d_from_contrib(self.X,5,1,6)\n",
    "            #print('conv1: ' + str(self.conv1.get_shape()))\n",
    "        \n",
    "        with tf.variable_scope('max_pooling1'):\n",
    "            self.maxpool1 = max_pool(self.conv1, 2, 2)\n",
    "            #print('maxpool1: ' + str(self.maxpool1.get_shape()))\n",
    "            \n",
    "        with tf.variable_scope('conv2'):\n",
    "            self.conv2 = conv2d_from_contrib(self.maxpool1,5,1,16)\n",
    "            #print('conv2: ' + str(self.conv2.get_shape()))\n",
    "        \n",
    "        with tf.variable_scope('max_pooling2'):\n",
    "            self.maxpool2 = max_pool(self.conv2, 2, 2)\n",
    "            #print('maxpool2: ' + str(self.maxpool2.get_shape()))\n",
    "        \n",
    "        with tf.variable_scope('conv3'):\n",
    "            self.conv3 = conv2d_from_contrib(self.maxpool2,5,1,120)\n",
    "            #print('conv3: ' + str(self.conv3.get_shape()))\n",
    "        \n",
    "        self.flat = tf.contrib.layers.flatten(self.conv3)\n",
    "        \n",
    "        with tf.variable_scope('fc1'):\n",
    "            self.fc1 = fully_connected(self.flat, 84, weight_init = tf.random_normal_initializer(0.0, 0.1))\n",
    "            #print('fc1 ' + str(self.fc1.get_shape()))\n",
    "        \n",
    "        with tf.variable_scope('fc2'):\n",
    "            self.fc2 = fully_connected(self.fc1, 10, weight_init = tf.random_normal_initializer(0.0, 0.1))\n",
    "            #print('fc2 ' + str(self.fc2.get_shape()))\n",
    "        \n",
    "        return self.fc2\n",
    "    \n",
    "    def _input_ops(self):\n",
    "        self.X = tf.placeholder(tf.float32, [None, 28, 28, 1])\n",
    "        self.Y = tf.placeholder(tf.int64, [None])\n",
    "        self.is_train = tf.placeholder(tf.bool)\n",
    "        self.keep_prob = tf.placeholder(tf.float32)\n",
    "        \n",
    "    def _build_optimizer(self):\n",
    "        step = tf.Variable(0)\n",
    "        self.lr = tf.train.exponential_decay(1e-4,step,500,0.96)\n",
    "        self.train_op = tf.train.AdamOptimizer(learning_rate = self.lr).minimize(self.loss_op, global_step = step)\n",
    "        \n",
    "    def _loss(self,labels,logits):\n",
    "        self.loss_op = tf.reduce_sum(tf.nn.softmax_cross_entropy_with_logits(labels = labels, logits = logits)) \n",
    "    \n",
    "    def _build_model(self):\n",
    "        self._input_ops()\n",
    "        labels = tf.one_hot(self.Y, 10)\n",
    "        logits = self._model()\n",
    "        self._loss(labels, logits)\n",
    "        self._build_optimizer()\n",
    "        \n",
    "        predict = tf.argmax(logits, 1)\n",
    "        correct = tf.equal(predict, self.Y)\n",
    "        self.accuracy_op = tf.reduce_mean(tf.cast(correct, tf.float32))\n",
    "    \n",
    "    def train(self, sess, X_train, Y_train, X_val, Y_val):\n",
    "        sess.run(tf.global_variables_initializer())\n",
    "        step = 0\n",
    "        losses = []\n",
    "        accuracies = []\n",
    "        train_acc = []\n",
    "        test_acc = []\n",
    "        \n",
    "        print('-' * 5 + '  Start training  ' + '-' * 5)\n",
    "        for epoch in range(self.num_epoch):\n",
    "            print('train for epoch %d' % epoch)\n",
    "            for i in range(num_training // self.batch_size):\n",
    "                X_ = X_train[i * self.batch_size:(i + 1) * self.batch_size][:]\n",
    "                Y_ = Y_train[i * self.batch_size:(i + 1) * self.batch_size]              \n",
    "                feed_dict = {self.X: X_, self.Y:Y_}              \n",
    "                fetches = [self.train_op, self.loss_op, self.accuracy_op]\n",
    "                _, loss, accuracy = sess.run(fetches, feed_dict=feed_dict)\n",
    "                losses.append(loss)\n",
    "                accuracies.append(accuracy)\n",
    "                if step % self.log_step == 0:\n",
    "                    print('iteration (%d): loss = %.3f, accuracy = %.3f' %\n",
    "                        (step, loss, accuracy))\n",
    "                step += 1\n",
    "#             # Graph 1. X: epoch, Y: training loss\n",
    "#             x1 = np.linspace(0, epoch + 1, len(losses))\n",
    "#             plt.plot(x1, np.array(losses), label='epoch-loss')\n",
    "#             plt.xlabel('epoch')\n",
    "#             plt.ylabel('training loss')\n",
    "#             plt.legend()\n",
    "#             plt.grid(True)\n",
    "#             plt.show()\n",
    "#             # Graph 2. X: epoch, Y: training accuracy\n",
    "#             x2 = np.linspace(0, epoch + 1, len(losses))\n",
    "#             plt.plot(x2, np.array(accuracies), label='epoch-accuracy_train')\n",
    "#             plt.xlabel('epoch')\n",
    "#             plt.ylabel('accuracy')\n",
    "#             plt.legend()\n",
    "#             plt.grid(True)\n",
    "#             plt.show()\n",
    "            # Print validation results\n",
    "            print('validation for epoch %d' % epoch)\n",
    "            train_accuracy = self.evaluate(sess, X_train, Y_train)\n",
    "            val_accuracy = self.evaluate(sess, X_val, Y_val)\n",
    "            if val_accuracy>self.highest_accuracy:\n",
    "                self.highest_accuracy=val_accuracy\n",
    "            train_acc.append(train_accuracy)\n",
    "            test_acc.append(val_accuracy)           \n",
    "            print('-  epoch %d: Train accuracy = %.3f' % (epoch, train_accuracy))\n",
    "            print('-  epoch %d: Test accuracy = %.3f' % (epoch, val_accuracy))\n",
    "        # Graph 3. X: epoch, Y: Train_accuray\n",
    "        x1 = np.linspace(0, epoch + 1, len(train_acc))\n",
    "        plt.plot(x1, np.array(train_acc), label='epoch-train_accuracy')\n",
    "        plt.xlabel('epoch')\n",
    "        plt.ylabel('training accuracy')\n",
    "        plt.legend()\n",
    "        plt.grid(True)\n",
    "        plt.show()\n",
    "        # Graph 4. X: epoch, Y: Test_accuray\n",
    "        x1 = np.linspace(0, epoch + 1, len(test_acc))\n",
    "        plt.plot(x1, np.array(test_acc), label='epoch-test_accuracy')\n",
    "        plt.xlabel('epoch')\n",
    "        plt.ylabel('testing accuracy')\n",
    "        plt.legend()\n",
    "        plt.grid(True)\n",
    "        plt.show()\n",
    "        print(\"Highest test accuracy is: \" + str(self.highest_accuracy))\n",
    "            \n",
    "            \n",
    "            \n",
    "            \n",
    "                \n",
    "    def evaluate(self, sess, X_eval, Y_eval):\n",
    "        eval_accuracy = 0.0\n",
    "        eval_iter = 0\n",
    "        for i in range(X_eval.shape[0] // self.batch_size):\n",
    "            X_ = X_eval[i * self.batch_size:(i + 1) * self.batch_size][:]\n",
    "            Y_ = Y_eval[i * self.batch_size:(i + 1) * self.batch_size]\n",
    "            feed_dict = {self.X:X_, self.Y:Y_}\n",
    "            accuracy = sess.run(self.accuracy_op, feed_dict=feed_dict)\n",
    "            eval_accuracy += accuracy\n",
    "            eval_iter += 1\n",
    "        return eval_accuracy / eval_iter  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "num_training = 60000\n",
    "num_test = 10000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "tf.reset_default_graph()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---Le_Net5---\n",
      "intput: (?, 28, 28, 1)\n",
      "-----  Start training  -----\n",
      "train for epoch 0\n",
      "iteration (0): loss = 5384.911, accuracy = 0.062\n",
      "iteration (50): loss = 229.132, accuracy = 0.766\n",
      "iteration (100): loss = 115.726, accuracy = 0.891\n",
      "iteration (150): loss = 201.952, accuracy = 0.797\n",
      "iteration (200): loss = 160.707, accuracy = 0.828\n",
      "iteration (250): loss = 211.062, accuracy = 0.766\n",
      "iteration (300): loss = 43.853, accuracy = 0.938\n",
      "iteration (350): loss = 33.674, accuracy = 0.953\n",
      "iteration (400): loss = 39.967, accuracy = 0.953\n",
      "iteration (450): loss = 67.472, accuracy = 0.906\n",
      "iteration (500): loss = 74.466, accuracy = 0.875\n",
      "iteration (550): loss = 85.065, accuracy = 0.844\n",
      "iteration (600): loss = 34.302, accuracy = 0.938\n",
      "iteration (650): loss = 99.410, accuracy = 0.906\n",
      "iteration (700): loss = 63.873, accuracy = 0.922\n",
      "iteration (750): loss = 44.619, accuracy = 0.938\n",
      "iteration (800): loss = 130.743, accuracy = 0.875\n",
      "iteration (850): loss = 31.329, accuracy = 0.953\n",
      "iteration (900): loss = 63.275, accuracy = 0.906\n",
      "validation for epoch 0\n",
      "-  epoch 0: Train accuracy = 0.940\n",
      "-  epoch 0: Test accuracy = 0.941\n",
      "train for epoch 1\n",
      "iteration (950): loss = 35.686, accuracy = 0.922\n",
      "iteration (1000): loss = 19.696, accuracy = 0.938\n",
      "iteration (1050): loss = 66.547, accuracy = 0.859\n",
      "iteration (1100): loss = 33.056, accuracy = 0.969\n",
      "iteration (1150): loss = 63.061, accuracy = 0.891\n",
      "iteration (1200): loss = 25.793, accuracy = 0.938\n",
      "iteration (1250): loss = 38.933, accuracy = 0.938\n",
      "iteration (1300): loss = 1.153, accuracy = 0.984\n",
      "iteration (1350): loss = 8.364, accuracy = 0.953\n",
      "iteration (1400): loss = 26.426, accuracy = 0.969\n",
      "iteration (1450): loss = 66.807, accuracy = 0.953\n",
      "iteration (1500): loss = 37.334, accuracy = 0.906\n",
      "iteration (1550): loss = 48.607, accuracy = 0.906\n",
      "iteration (1600): loss = 76.169, accuracy = 0.891\n",
      "iteration (1650): loss = 23.414, accuracy = 0.906\n",
      "iteration (1700): loss = 4.785, accuracy = 0.984\n",
      "iteration (1750): loss = 13.629, accuracy = 0.984\n",
      "iteration (1800): loss = 16.769, accuracy = 0.969\n",
      "iteration (1850): loss = 0.003, accuracy = 1.000\n",
      "validation for epoch 1\n",
      "-  epoch 1: Train accuracy = 0.956\n",
      "-  epoch 1: Test accuracy = 0.950\n",
      "train for epoch 2\n",
      "iteration (1900): loss = 4.840, accuracy = 0.984\n",
      "iteration (1950): loss = 15.960, accuracy = 0.953\n",
      "iteration (2000): loss = 37.282, accuracy = 0.938\n",
      "iteration (2050): loss = 4.690, accuracy = 0.984\n",
      "iteration (2100): loss = 28.200, accuracy = 0.953\n",
      "iteration (2150): loss = 24.456, accuracy = 0.922\n",
      "iteration (2200): loss = 5.492, accuracy = 0.953\n",
      "iteration (2250): loss = 4.972, accuracy = 0.969\n",
      "iteration (2300): loss = 20.668, accuracy = 0.953\n",
      "iteration (2350): loss = 1.903, accuracy = 0.984\n",
      "iteration (2400): loss = 5.734, accuracy = 0.984\n",
      "iteration (2450): loss = 9.724, accuracy = 0.969\n",
      "iteration (2500): loss = 4.151, accuracy = 0.969\n",
      "iteration (2550): loss = 1.635, accuracy = 0.984\n",
      "iteration (2600): loss = 6.023, accuracy = 0.984\n",
      "iteration (2650): loss = 12.245, accuracy = 0.969\n",
      "iteration (2700): loss = 42.066, accuracy = 0.953\n",
      "iteration (2750): loss = 20.845, accuracy = 0.984\n",
      "iteration (2800): loss = 10.229, accuracy = 0.938\n",
      "validation for epoch 2\n",
      "-  epoch 2: Train accuracy = 0.957\n",
      "-  epoch 2: Test accuracy = 0.949\n",
      "train for epoch 3\n",
      "iteration (2850): loss = 5.381, accuracy = 0.984\n",
      "iteration (2900): loss = 13.106, accuracy = 0.953\n",
      "iteration (2950): loss = 19.639, accuracy = 0.969\n",
      "iteration (3000): loss = 3.173, accuracy = 0.984\n",
      "iteration (3050): loss = 4.478, accuracy = 0.984\n",
      "iteration (3100): loss = 19.079, accuracy = 0.953\n",
      "iteration (3150): loss = 0.107, accuracy = 1.000\n",
      "iteration (3200): loss = 23.271, accuracy = 0.969\n",
      "iteration (3250): loss = 5.002, accuracy = 0.953\n",
      "iteration (3300): loss = 22.638, accuracy = 0.922\n",
      "iteration (3350): loss = 10.979, accuracy = 0.969\n",
      "iteration (3400): loss = 12.329, accuracy = 0.953\n",
      "iteration (3450): loss = 0.346, accuracy = 1.000\n",
      "iteration (3500): loss = 20.610, accuracy = 0.969\n",
      "iteration (3550): loss = 19.532, accuracy = 0.969\n",
      "iteration (3600): loss = 2.783, accuracy = 0.984\n",
      "iteration (3650): loss = 6.771, accuracy = 0.984\n",
      "iteration (3700): loss = 1.945, accuracy = 0.984\n",
      "validation for epoch 3\n",
      "-  epoch 3: Train accuracy = 0.968\n",
      "-  epoch 3: Test accuracy = 0.961\n",
      "train for epoch 4\n",
      "iteration (3750): loss = 29.266, accuracy = 0.938\n",
      "iteration (3800): loss = 5.766, accuracy = 0.984\n",
      "iteration (3850): loss = 0.052, accuracy = 1.000\n",
      "iteration (3900): loss = 7.375, accuracy = 0.969\n",
      "iteration (3950): loss = 1.121, accuracy = 1.000\n",
      "iteration (4000): loss = 10.565, accuracy = 0.953\n",
      "iteration (4050): loss = 13.865, accuracy = 0.938\n",
      "iteration (4100): loss = 6.175, accuracy = 0.969\n",
      "iteration (4150): loss = 0.084, accuracy = 1.000\n",
      "iteration (4200): loss = 7.088, accuracy = 0.984\n",
      "iteration (4250): loss = 6.066, accuracy = 0.969\n",
      "iteration (4300): loss = 0.296, accuracy = 1.000\n",
      "iteration (4350): loss = 0.728, accuracy = 1.000\n",
      "iteration (4400): loss = 3.531, accuracy = 0.969\n",
      "iteration (4450): loss = 16.776, accuracy = 0.984\n",
      "iteration (4500): loss = 3.010, accuracy = 0.984\n",
      "iteration (4550): loss = 1.669, accuracy = 0.984\n",
      "iteration (4600): loss = 0.953, accuracy = 1.000\n",
      "iteration (4650): loss = 17.405, accuracy = 0.922\n",
      "validation for epoch 4\n",
      "-  epoch 4: Train accuracy = 0.974\n",
      "-  epoch 4: Test accuracy = 0.963\n",
      "train for epoch 5\n",
      "iteration (4700): loss = 6.379, accuracy = 0.953\n",
      "iteration (4750): loss = 10.733, accuracy = 0.953\n",
      "iteration (4800): loss = 0.243, accuracy = 1.000\n",
      "iteration (4850): loss = 0.012, accuracy = 1.000\n",
      "iteration (4900): loss = 4.663, accuracy = 0.984\n",
      "iteration (4950): loss = 1.929, accuracy = 0.984\n",
      "iteration (5000): loss = 13.488, accuracy = 0.938\n",
      "iteration (5050): loss = 0.729, accuracy = 1.000\n",
      "iteration (5100): loss = 41.830, accuracy = 0.953\n",
      "iteration (5150): loss = 1.000, accuracy = 1.000\n",
      "iteration (5200): loss = 0.001, accuracy = 1.000\n",
      "iteration (5250): loss = 0.139, accuracy = 1.000\n",
      "iteration (5300): loss = 18.483, accuracy = 0.969\n",
      "iteration (5350): loss = 11.220, accuracy = 0.969\n",
      "iteration (5400): loss = 4.279, accuracy = 0.969\n",
      "iteration (5450): loss = 11.158, accuracy = 0.938\n",
      "iteration (5500): loss = 2.267, accuracy = 0.984\n",
      "iteration (5550): loss = 2.282, accuracy = 0.984\n",
      "iteration (5600): loss = 0.016, accuracy = 1.000\n",
      "validation for epoch 5\n",
      "-  epoch 5: Train accuracy = 0.982\n",
      "-  epoch 5: Test accuracy = 0.970\n",
      "train for epoch 6\n",
      "iteration (5650): loss = 1.661, accuracy = 0.984\n",
      "iteration (5700): loss = 0.429, accuracy = 1.000\n",
      "iteration (5750): loss = 26.084, accuracy = 0.922\n",
      "iteration (5800): loss = 6.077, accuracy = 0.984\n",
      "iteration (5850): loss = 0.789, accuracy = 1.000\n",
      "iteration (5900): loss = 9.585, accuracy = 0.969\n",
      "iteration (5950): loss = 3.312, accuracy = 0.969\n",
      "iteration (6000): loss = 1.074, accuracy = 0.984\n",
      "iteration (6050): loss = 0.733, accuracy = 1.000\n",
      "iteration (6100): loss = 3.431, accuracy = 0.984\n",
      "iteration (6150): loss = 0.332, accuracy = 1.000\n",
      "iteration (6200): loss = 4.403, accuracy = 0.969\n",
      "iteration (6250): loss = 1.437, accuracy = 0.984\n",
      "iteration (6300): loss = 1.041, accuracy = 0.984\n",
      "iteration (6350): loss = 0.016, accuracy = 1.000\n",
      "iteration (6400): loss = 0.627, accuracy = 1.000\n",
      "iteration (6450): loss = 3.817, accuracy = 0.984\n",
      "iteration (6500): loss = 0.318, accuracy = 1.000\n",
      "iteration (6550): loss = 3.605, accuracy = 0.984\n",
      "validation for epoch 6\n",
      "-  epoch 6: Train accuracy = 0.986\n",
      "-  epoch 6: Test accuracy = 0.973\n",
      "train for epoch 7\n",
      "iteration (6600): loss = 0.710, accuracy = 1.000\n",
      "iteration (6650): loss = 3.791, accuracy = 0.969\n",
      "iteration (6700): loss = 0.207, accuracy = 1.000\n",
      "iteration (6750): loss = 0.475, accuracy = 1.000\n",
      "iteration (6800): loss = 1.415, accuracy = 0.984\n",
      "iteration (6850): loss = 0.121, accuracy = 1.000\n",
      "iteration (6900): loss = 0.015, accuracy = 1.000\n",
      "iteration (6950): loss = 0.489, accuracy = 1.000\n",
      "iteration (7000): loss = 5.599, accuracy = 0.984\n",
      "iteration (7050): loss = 11.308, accuracy = 0.969\n",
      "iteration (7100): loss = 0.197, accuracy = 1.000\n",
      "iteration (7150): loss = 7.307, accuracy = 0.984\n",
      "iteration (7200): loss = 0.064, accuracy = 1.000\n",
      "iteration (7250): loss = 0.132, accuracy = 1.000\n",
      "iteration (7300): loss = 2.074, accuracy = 0.984\n",
      "iteration (7350): loss = 0.036, accuracy = 1.000\n",
      "iteration (7400): loss = 2.587, accuracy = 0.969\n",
      "iteration (7450): loss = 0.494, accuracy = 1.000\n",
      "validation for epoch 7\n",
      "-  epoch 7: Train accuracy = 0.986\n",
      "-  epoch 7: Test accuracy = 0.974\n",
      "train for epoch 8\n",
      "iteration (7500): loss = 0.105, accuracy = 1.000\n",
      "iteration (7550): loss = 2.989, accuracy = 0.984\n",
      "iteration (7600): loss = 6.656, accuracy = 0.953\n",
      "iteration (7650): loss = 0.409, accuracy = 1.000\n",
      "iteration (7700): loss = 3.073, accuracy = 0.984\n",
      "iteration (7750): loss = 0.100, accuracy = 1.000\n",
      "iteration (7800): loss = 4.741, accuracy = 0.969\n",
      "iteration (7850): loss = 0.680, accuracy = 1.000\n",
      "iteration (7900): loss = 0.204, accuracy = 1.000\n",
      "iteration (7950): loss = 0.020, accuracy = 1.000\n",
      "iteration (8000): loss = 0.660, accuracy = 1.000\n",
      "iteration (8050): loss = 2.948, accuracy = 0.984\n",
      "iteration (8100): loss = 0.344, accuracy = 1.000\n",
      "iteration (8150): loss = 1.719, accuracy = 0.984\n",
      "iteration (8200): loss = 0.577, accuracy = 1.000\n",
      "iteration (8250): loss = 7.636, accuracy = 0.984\n",
      "iteration (8300): loss = 1.681, accuracy = 0.984\n",
      "iteration (8350): loss = 0.090, accuracy = 1.000\n",
      "iteration (8400): loss = 0.056, accuracy = 1.000\n",
      "validation for epoch 8\n",
      "-  epoch 8: Train accuracy = 0.992\n",
      "-  epoch 8: Test accuracy = 0.976\n",
      "train for epoch 9\n",
      "iteration (8450): loss = 0.909, accuracy = 1.000\n",
      "iteration (8500): loss = 1.853, accuracy = 0.969\n",
      "iteration (8550): loss = 2.681, accuracy = 0.984\n",
      "iteration (8600): loss = 0.051, accuracy = 1.000\n",
      "iteration (8650): loss = 1.826, accuracy = 0.984\n",
      "iteration (8700): loss = 0.247, accuracy = 1.000\n",
      "iteration (8750): loss = 0.670, accuracy = 1.000\n",
      "iteration (8800): loss = 0.107, accuracy = 1.000\n",
      "iteration (8850): loss = 16.740, accuracy = 0.969\n",
      "iteration (8900): loss = 3.199, accuracy = 0.969\n",
      "iteration (8950): loss = 0.005, accuracy = 1.000\n",
      "iteration (9000): loss = 0.030, accuracy = 1.000\n",
      "iteration (9050): loss = 0.181, accuracy = 1.000\n",
      "iteration (9100): loss = 1.797, accuracy = 0.984\n",
      "iteration (9150): loss = 0.024, accuracy = 1.000\n",
      "iteration (9200): loss = 1.393, accuracy = 0.984\n",
      "iteration (9250): loss = 4.690, accuracy = 0.984\n",
      "iteration (9300): loss = 0.215, accuracy = 1.000\n",
      "iteration (9350): loss = 0.003, accuracy = 1.000\n",
      "validation for epoch 9\n",
      "-  epoch 9: Train accuracy = 0.992\n",
      "-  epoch 9: Test accuracy = 0.975\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYwAAAEKCAYAAAAB0GKPAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\ngg==\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x1b3663af828>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAZIAAAEKCAYAAAA4t9PUAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nxhhTIZZIjDHGVMj/B+pyrMUS6b0lAAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x1b3663c05c0>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Highest test accuracy is: 0.976462339744\n",
      "***** Last accuracy: 0.975\n"
     ]
    }
   ],
   "source": [
    "with tf.Session() as sess:\n",
    "    model = Le_Net5()\n",
    "    model.train(sess, X_train, Y_train, X_test, Y_test)\n",
    "    accuracy = model.evaluate(sess, X_test, Y_test)\n",
    "    print('***** Last accuracy: %.3f' % accuracy)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "class Super_Le_Net5(object):\n",
    "    def __init__(self):\n",
    "        self.num_epoch = 50\n",
    "        self.batch_size = 128\n",
    "        self.log_step = 50\n",
    "        self.highest_accuracy=0.\n",
    "        self._build_model()\n",
    "        \n",
    "              \n",
    "    def _model(self):\n",
    "        print('---Super Le_Net5---')\n",
    "        \n",
    "        print('intput: ' + str(self.X.get_shape()))\n",
    "        \n",
    "        with tf.variable_scope('conv1'):\n",
    "            self.conv1 = conv2d_from_contrib(self.X,4,2,64)\n",
    "            print('conv1: ' + str(self.conv1.get_shape()))\n",
    "        with tf.variable_scope('conv2'):\n",
    "            self.conv2 = conv2d_from_contrib(self.conv1,4,2,128)\n",
    "            print('conv2: ' + str(self.conv2.get_shape()))\n",
    "        with tf.variable_scope('conv3'):\n",
    "            self.conv3 = conv2d_from_contrib(self.conv2,4,2,384)\n",
    "            print('conv3: ' + str(self.conv3.get_shape()))\n",
    "            \n",
    "        self.flat = tf.contrib.layers.flatten(self.conv3)\n",
    "            \n",
    "        with tf.variable_scope('fc1'):\n",
    "            self.fc1 = fully_connected(self.flat, 384, weight_init = tf.contrib.layers.xavier_initializer())\n",
    "            print('fc1 ' + str(self.fc1.get_shape()))\n",
    "            \n",
    "        with tf.variable_scope('fc2'):\n",
    "            self.fc2 = fully_connected(self.fc1, 192, weight_init = tf.contrib.layers.xavier_initializer())\n",
    "            print('fc2 ' + str(self.fc2.get_shape())) \n",
    "        \n",
    "        with tf.variable_scope('fc3'):\n",
    "            self.fc3 = fully_connected(self.fc2, 10, weight_init = tf.contrib.layers.xavier_initializer())\n",
    "            print('fc3 ' + str(self.fc3.get_shape()))    \n",
    "                      \n",
    "        return self.fc3\n",
    "    \n",
    "    def _input_ops(self):\n",
    "        self.X = tf.placeholder(tf.float32, [None, 28, 28, 1])\n",
    "        self.Y = tf.placeholder(tf.int64, [None])\n",
    "        self.is_train = tf.placeholder(tf.bool)\n",
    "        self.keep_prob = tf.placeholder(tf.float32)\n",
    "        \n",
    "    def _build_optimizer(self):\n",
    "        step = tf.Variable(0)\n",
    "        self.lr = tf.train.exponential_decay(1e-4,step,500,0.96)\n",
    "        self.train_op = tf.train.AdamOptimizer(learning_rate = self.lr).minimize(self.loss_op, global_step = step)\n",
    "        \n",
    "    def _loss(self,labels,logits):\n",
    "        self.loss_op = tf.reduce_sum(tf.nn.softmax_cross_entropy_with_logits(labels = labels, logits = logits)) \n",
    "    \n",
    "    def _build_model(self):\n",
    "        self._input_ops()\n",
    "        labels = tf.one_hot(self.Y, 10)\n",
    "        logits = self._model()\n",
    "        self._loss(labels, logits)\n",
    "        self._build_optimizer()\n",
    "        \n",
    "        predict = tf.argmax(logits, 1)\n",
    "        correct = tf.equal(predict, self.Y)\n",
    "        self.accuracy_op = tf.reduce_mean(tf.cast(correct, tf.float32))\n",
    "    \n",
    "    def train(self, sess, X_train, Y_train, X_val, Y_val):\n",
    "        sess.run(tf.global_variables_initializer())\n",
    "        step = 0\n",
    "        losses = []\n",
    "        accuracies = []\n",
    "        train_acc = []\n",
    "        test_acc = []\n",
    "        \n",
    "        print('-' * 5 + '  Start training  ' + '-' * 5)\n",
    "        for epoch in range(self.num_epoch):\n",
    "            print('train for epoch %d' % epoch)\n",
    "            for i in range(num_training // self.batch_size):\n",
    "                X_ = X_train[i * self.batch_size:(i + 1) * self.batch_size][:]\n",
    "                Y_ = Y_train[i * self.batch_size:(i + 1) * self.batch_size]              \n",
    "                feed_dict = {self.X: X_, self.Y:Y_}              \n",
    "                fetches = [self.train_op, self.loss_op, self.accuracy_op]\n",
    "                _, loss, accuracy = sess.run(fetches, feed_dict=feed_dict)\n",
    "                losses.append(loss)\n",
    "                accuracies.append(accuracy)\n",
    "                if step % self.log_step == 0:\n",
    "                    print('iteration (%d): loss = %.3f, accuracy = %.3f' %\n",
    "                        (step, loss, accuracy))\n",
    "                step += 1\n",
    "#             # Graph 1. X: epoch, Y: training loss\n",
    "#             x1 = np.linspace(0, epoch + 1, len(losses))\n",
    "#             plt.plot(x1, np.array(losses), label='epoch-loss')\n",
    "#             plt.xlabel('epoch')\n",
    "#             plt.ylabel('training loss')\n",
    "#             plt.legend()\n",
    "#             plt.grid(True)\n",
    "#             plt.show()\n",
    "#             # Graph 2. X: epoch, Y: training accuracy\n",
    "#             x2 = np.linspace(0, epoch + 1, len(losses))\n",
    "#             plt.plot(x2, np.array(accuracies), label='epoch-accuracy_train')\n",
    "#             plt.xlabel('epoch')\n",
    "#             plt.ylabel('accuracy')\n",
    "#             plt.legend()\n",
    "#             plt.grid(True)\n",
    "#             plt.show()\n",
    "            # Print validation results\n",
    "            print('validation for epoch %d' % epoch)\n",
    "            train_accuracy = self.evaluate(sess, X_train, Y_train)\n",
    "            val_accuracy = self.evaluate(sess, X_val, Y_val)\n",
    "            if val_accuracy>self.highest_accuracy:\n",
    "                self.highest_accuracy = val_accuracy\n",
    "            train_acc.append(train_accuracy)\n",
    "            test_acc.append(val_accuracy)           \n",
    "            print('-  epoch %d: Train accuracy = %.3f' % (epoch, train_accuracy))\n",
    "            print('-  epoch %d: Test accuracy = %.3f' % (epoch, val_accuracy))\n",
    "        # Graph 3. X: epoch, Y: Train_accuray\n",
    "        x1 = np.linspace(0, epoch + 1, len(train_acc))\n",
    "        plt.plot(x1, np.array(train_acc), label='epoch-train_accuracy')\n",
    "        plt.xlabel('epoch')\n",
    "        plt.ylabel('training accuracy')\n",
    "        plt.legend()\n",
    "        plt.grid(True)\n",
    "        plt.show()\n",
    "        # Graph 4. X: epoch, Y: Test_accuray\n",
    "        x1 = np.linspace(0, epoch + 1, len(test_acc))\n",
    "        plt.plot(x1, np.array(test_acc), label='epoch-test_accuracy')\n",
    "        plt.xlabel('epoch')\n",
    "        plt.ylabel('testing accuracy')\n",
    "        plt.legend()\n",
    "        plt.grid(True)\n",
    "        plt.show()\n",
    "        print(\"Highest test accuracy is: \" + str(self.highest_accuracy))\n",
    "                \n",
    "    def evaluate(self, sess, X_eval, Y_eval):\n",
    "        eval_accuracy = 0.0\n",
    "        eval_iter = 0\n",
    "        for i in range(X_eval.shape[0] // self.batch_size):\n",
    "            X_ = X_eval[i * self.batch_size:(i + 1) * self.batch_size][:]\n",
    "            Y_ = Y_eval[i * self.batch_size:(i + 1) * self.batch_size]\n",
    "            feed_dict = {self.X:X_, self.Y:Y_}\n",
    "            accuracy = sess.run(self.accuracy_op, feed_dict=feed_dict)\n",
    "            eval_accuracy += accuracy\n",
    "            eval_iter += 1\n",
    "        return eval_accuracy / eval_iter\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---Super Le_Net5---\n",
      "intput: (?, 28, 28, 1)\n",
      "conv1: (?, 14, 14, 64)\n",
      "conv2: (?, 7, 7, 128)\n",
      "conv3: (?, 4, 4, 384)\n",
      "fc1 (?, 384)\n",
      "fc2 (?, 192)\n",
      "fc3 (?, 10)\n",
      "-----  Start training  -----\n",
      "train for epoch 0\n",
      "iteration (0): loss = 1375.970, accuracy = 0.133\n",
      "iteration (50): loss = 62.536, accuracy = 0.883\n",
      "iteration (100): loss = 22.567, accuracy = 0.938\n",
      "iteration (150): loss = 16.488, accuracy = 0.977\n",
      "iteration (200): loss = 15.125, accuracy = 0.945\n",
      "iteration (250): loss = 20.960, accuracy = 0.938\n",
      "iteration (300): loss = 17.990, accuracy = 0.969\n",
      "iteration (350): loss = 22.159, accuracy = 0.953\n",
      "iteration (400): loss = 15.216, accuracy = 0.953\n",
      "iteration (450): loss = 11.306, accuracy = 0.953\n",
      "validation for epoch 0\n",
      "-  epoch 0: Train accuracy = 0.974\n",
      "-  epoch 0: Test accuracy = 0.963\n",
      "train for epoch 1\n",
      "iteration (500): loss = 13.028, accuracy = 0.969\n",
      "iteration (550): loss = 1.105, accuracy = 1.000\n",
      "iteration (600): loss = 6.403, accuracy = 0.992\n",
      "iteration (650): loss = 4.256, accuracy = 0.992\n",
      "iteration (700): loss = 6.347, accuracy = 0.977\n",
      "iteration (750): loss = 15.794, accuracy = 0.984\n",
      "iteration (800): loss = 11.112, accuracy = 0.969\n",
      "iteration (850): loss = 20.225, accuracy = 0.953\n",
      "iteration (900): loss = 4.981, accuracy = 0.992\n",
      "validation for epoch 1\n",
      "-  epoch 1: Train accuracy = 0.992\n",
      "-  epoch 1: Test accuracy = 0.979\n",
      "train for epoch 2\n",
      "iteration (950): loss = 0.702, accuracy = 1.000\n",
      "iteration (1000): loss = 7.256, accuracy = 0.977\n",
      "iteration (1050): loss = 1.337, accuracy = 1.000\n",
      "iteration (1100): loss = 2.125, accuracy = 0.992\n",
      "iteration (1150): loss = 1.233, accuracy = 1.000\n",
      "iteration (1200): loss = 1.191, accuracy = 1.000\n",
      "iteration (1250): loss = 4.753, accuracy = 0.992\n",
      "iteration (1300): loss = 2.177, accuracy = 0.992\n",
      "iteration (1350): loss = 1.763, accuracy = 0.992\n",
      "iteration (1400): loss = 1.186, accuracy = 1.000\n",
      "validation for epoch 2\n",
      "-  epoch 2: Train accuracy = 0.994\n",
      "-  epoch 2: Test accuracy = 0.980\n",
      "train for epoch 3\n",
      "iteration (1450): loss = 1.092, accuracy = 1.000\n",
      "iteration (1500): loss = 2.475, accuracy = 0.992\n",
      "iteration (1550): loss = 0.186, accuracy = 1.000\n",
      "iteration (1600): loss = 1.089, accuracy = 1.000\n",
      "iteration (1650): loss = 0.785, accuracy = 1.000\n",
      "iteration (1700): loss = 0.249, accuracy = 1.000\n",
      "iteration (1750): loss = 1.201, accuracy = 1.000\n",
      "iteration (1800): loss = 0.553, accuracy = 1.000\n",
      "iteration (1850): loss = 0.220, accuracy = 1.000\n",
      "validation for epoch 3\n",
      "-  epoch 3: Train accuracy = 0.995\n",
      "-  epoch 3: Test accuracy = 0.980\n",
      "train for epoch 4\n",
      "iteration (1900): loss = 0.524, accuracy = 1.000\n",
      "iteration (1950): loss = 0.452, accuracy = 1.000\n",
      "iteration (2000): loss = 1.030, accuracy = 0.992\n",
      "iteration (2050): loss = 0.165, accuracy = 1.000\n",
      "iteration (2100): loss = 1.133, accuracy = 1.000\n",
      "iteration (2150): loss = 0.760, accuracy = 1.000\n",
      "iteration (2200): loss = 0.192, accuracy = 1.000\n",
      "iteration (2250): loss = 0.201, accuracy = 1.000\n",
      "iteration (2300): loss = 3.349, accuracy = 0.984\n",
      "validation for epoch 4\n",
      "-  epoch 4: Train accuracy = 0.995\n",
      "-  epoch 4: Test accuracy = 0.980\n",
      "train for epoch 5\n",
      "iteration (2350): loss = 6.015, accuracy = 0.977\n",
      "iteration (2400): loss = 3.402, accuracy = 0.992\n",
      "iteration (2450): loss = 0.297, accuracy = 1.000\n",
      "iteration (2500): loss = 0.131, accuracy = 1.000\n",
      "iteration (2550): loss = 0.534, accuracy = 1.000\n",
      "iteration (2600): loss = 0.393, accuracy = 1.000\n",
      "iteration (2650): loss = 0.960, accuracy = 1.000\n",
      "iteration (2700): loss = 2.087, accuracy = 0.992\n",
      "iteration (2750): loss = 0.541, accuracy = 1.000\n",
      "iteration (2800): loss = 0.472, accuracy = 1.000\n",
      "validation for epoch 5\n",
      "-  epoch 5: Train accuracy = 0.992\n",
      "-  epoch 5: Test accuracy = 0.979\n",
      "train for epoch 6\n",
      "iteration (2850): loss = 0.455, accuracy = 1.000\n",
      "iteration (2900): loss = 2.887, accuracy = 0.992\n",
      "iteration (2950): loss = 0.370, accuracy = 1.000\n",
      "iteration (3000): loss = 1.070, accuracy = 1.000\n",
      "iteration (3050): loss = 0.292, accuracy = 1.000\n",
      "iteration (3100): loss = 0.505, accuracy = 1.000\n",
      "iteration (3150): loss = 2.513, accuracy = 0.992\n",
      "iteration (3200): loss = 2.199, accuracy = 0.992\n",
      "iteration (3250): loss = 5.107, accuracy = 0.992\n",
      "validation for epoch 6\n",
      "-  epoch 6: Train accuracy = 0.995\n",
      "-  epoch 6: Test accuracy = 0.980\n",
      "train for epoch 7\n",
      "iteration (3300): loss = 0.056, accuracy = 1.000\n",
      "iteration (3350): loss = 0.640, accuracy = 1.000\n",
      "iteration (3400): loss = 2.258, accuracy = 0.992\n",
      "iteration (3450): loss = 0.766, accuracy = 1.000\n",
      "iteration (3500): loss = 0.246, accuracy = 1.000\n",
      "iteration (3550): loss = 0.035, accuracy = 1.000\n",
      "iteration (3600): loss = 0.110, accuracy = 1.000\n",
      "iteration (3650): loss = 0.451, accuracy = 1.000\n",
      "iteration (3700): loss = 0.104, accuracy = 1.000\n",
      "validation for epoch 7\n",
      "-  epoch 7: Train accuracy = 0.996\n",
      "-  epoch 7: Test accuracy = 0.981\n",
      "train for epoch 8\n",
      "iteration (3750): loss = 0.043, accuracy = 1.000\n",
      "iteration (3800): loss = 2.163, accuracy = 0.984\n",
      "iteration (3850): loss = 0.464, accuracy = 1.000\n",
      "iteration (3900): loss = 0.250, accuracy = 1.000\n",
      "iteration (3950): loss = 3.161, accuracy = 0.984\n",
      "iteration (4000): loss = 0.636, accuracy = 1.000\n",
      "iteration (4050): loss = 2.119, accuracy = 0.984\n",
      "iteration (4100): loss = 1.659, accuracy = 0.992\n",
      "iteration (4150): loss = 3.954, accuracy = 0.992\n",
      "iteration (4200): loss = 0.137, accuracy = 1.000\n",
      "validation for epoch 8\n",
      "-  epoch 8: Train accuracy = 0.998\n",
      "-  epoch 8: Test accuracy = 0.984\n",
      "train for epoch 9\n",
      "iteration (4250): loss = 0.011, accuracy = 1.000\n",
      "iteration (4300): loss = 0.269, accuracy = 1.000\n",
      "iteration (4350): loss = 1.415, accuracy = 0.992\n",
      "iteration (4400): loss = 0.041, accuracy = 1.000\n",
      "iteration (4450): loss = 0.061, accuracy = 1.000\n",
      "iteration (4500): loss = 6.808, accuracy = 0.984\n",
      "iteration (4550): loss = 0.012, accuracy = 1.000\n",
      "iteration (4600): loss = 0.050, accuracy = 1.000\n",
      "iteration (4650): loss = 0.152, accuracy = 1.000\n",
      "validation for epoch 9\n",
      "-  epoch 9: Train accuracy = 0.997\n",
      "-  epoch 9: Test accuracy = 0.984\n",
      "train for epoch 10\n",
      "iteration (4700): loss = 2.605, accuracy = 0.992\n",
      "iteration (4750): loss = 0.005, accuracy = 1.000\n",
      "iteration (4800): loss = 0.015, accuracy = 1.000\n",
      "iteration (4850): loss = 0.209, accuracy = 1.000\n",
      "iteration (4900): loss = 1.124, accuracy = 1.000\n",
      "iteration (4950): loss = 2.311, accuracy = 0.984\n",
      "iteration (5000): loss = 0.495, accuracy = 1.000\n",
      "iteration (5050): loss = 0.013, accuracy = 1.000\n",
      "iteration (5100): loss = 0.894, accuracy = 0.992\n",
      "validation for epoch 10\n",
      "-  epoch 10: Train accuracy = 0.997\n",
      "-  epoch 10: Test accuracy = 0.983\n",
      "train for epoch 11\n",
      "iteration (5150): loss = 0.022, accuracy = 1.000\n",
      "iteration (5200): loss = 1.859, accuracy = 0.992\n",
      "iteration (5250): loss = 0.178, accuracy = 1.000\n",
      "iteration (5300): loss = 0.046, accuracy = 1.000\n",
      "iteration (5350): loss = 0.197, accuracy = 1.000\n",
      "iteration (5400): loss = 0.181, accuracy = 1.000\n",
      "iteration (5450): loss = 0.137, accuracy = 1.000\n",
      "iteration (5500): loss = 0.078, accuracy = 1.000\n",
      "iteration (5550): loss = 0.180, accuracy = 1.000\n",
      "iteration (5600): loss = 0.157, accuracy = 1.000\n",
      "validation for epoch 11\n",
      "-  epoch 11: Train accuracy = 0.998\n",
      "-  epoch 11: Test accuracy = 0.985\n",
      "train for epoch 12\n",
      "iteration (5650): loss = 0.130, accuracy = 1.000\n",
      "iteration (5700): loss = 2.026, accuracy = 0.992\n",
      "iteration (5750): loss = 0.763, accuracy = 1.000\n",
      "iteration (5800): loss = 0.034, accuracy = 1.000\n",
      "iteration (5850): loss = 3.177, accuracy = 0.992\n",
      "iteration (5900): loss = 1.269, accuracy = 1.000\n",
      "iteration (5950): loss = 0.056, accuracy = 1.000\n",
      "iteration (6000): loss = 0.098, accuracy = 1.000\n",
      "iteration (6050): loss = 0.135, accuracy = 1.000\n",
      "validation for epoch 12\n",
      "-  epoch 12: Train accuracy = 0.998\n",
      "-  epoch 12: Test accuracy = 0.986\n",
      "train for epoch 13\n",
      "iteration (6100): loss = 0.067, accuracy = 1.000\n",
      "iteration (6150): loss = 0.209, accuracy = 1.000\n",
      "iteration (6200): loss = 0.031, accuracy = 1.000\n",
      "iteration (6250): loss = 0.175, accuracy = 1.000\n",
      "iteration (6300): loss = 0.059, accuracy = 1.000\n",
      "iteration (6350): loss = 0.021, accuracy = 1.000\n",
      "iteration (6400): loss = 0.015, accuracy = 1.000\n",
      "iteration (6450): loss = 0.022, accuracy = 1.000\n",
      "iteration (6500): loss = 0.007, accuracy = 1.000\n",
      "iteration (6550): loss = 0.127, accuracy = 1.000\n",
      "validation for epoch 13\n",
      "-  epoch 13: Train accuracy = 0.999\n",
      "-  epoch 13: Test accuracy = 0.987\n",
      "train for epoch 14\n",
      "iteration (6600): loss = 0.033, accuracy = 1.000\n",
      "iteration (6650): loss = 0.064, accuracy = 1.000\n",
      "iteration (6700): loss = 0.117, accuracy = 1.000\n",
      "iteration (6750): loss = 0.004, accuracy = 1.000\n",
      "iteration (6800): loss = 0.003, accuracy = 1.000\n",
      "iteration (6850): loss = 0.193, accuracy = 1.000\n",
      "iteration (6900): loss = 0.004, accuracy = 1.000\n",
      "iteration (6950): loss = 0.195, accuracy = 1.000\n",
      "iteration (7000): loss = 0.077, accuracy = 1.000\n",
      "validation for epoch 14\n",
      "-  epoch 14: Train accuracy = 1.000\n",
      "-  epoch 14: Test accuracy = 0.988\n",
      "train for epoch 15\n",
      "iteration (7050): loss = 0.001, accuracy = 1.000\n",
      "iteration (7100): loss = 0.022, accuracy = 1.000\n",
      "iteration (7150): loss = 2.123, accuracy = 0.992\n",
      "iteration (7200): loss = 0.157, accuracy = 1.000\n",
      "iteration (7250): loss = 0.056, accuracy = 1.000\n",
      "iteration (7300): loss = 0.002, accuracy = 1.000\n",
      "iteration (7350): loss = 0.031, accuracy = 1.000\n",
      "iteration (7400): loss = 0.001, accuracy = 1.000\n",
      "iteration (7450): loss = 0.001, accuracy = 1.000\n",
      "validation for epoch 15\n",
      "-  epoch 15: Train accuracy = 1.000\n",
      "-  epoch 15: Test accuracy = 0.989\n",
      "train for epoch 16\n",
      "iteration (7500): loss = 0.009, accuracy = 1.000\n",
      "iteration (7550): loss = 0.012, accuracy = 1.000\n",
      "iteration (7600): loss = 0.125, accuracy = 1.000\n",
      "iteration (7650): loss = 0.204, accuracy = 1.000\n",
      "iteration (7700): loss = 0.021, accuracy = 1.000\n",
      "iteration (7750): loss = 0.004, accuracy = 1.000\n",
      "iteration (7800): loss = 0.430, accuracy = 1.000\n",
      "iteration (7850): loss = 0.785, accuracy = 1.000\n",
      "iteration (7900): loss = 0.680, accuracy = 1.000\n",
      "iteration (7950): loss = 0.024, accuracy = 1.000\n",
      "validation for epoch 16\n",
      "-  epoch 16: Train accuracy = 0.998\n",
      "-  epoch 16: Test accuracy = 0.984\n",
      "train for epoch 17\n",
      "iteration (8000): loss = 2.313, accuracy = 0.984\n",
      "iteration (8050): loss = 0.025, accuracy = 1.000\n",
      "iteration (8100): loss = 0.011, accuracy = 1.000\n",
      "iteration (8150): loss = 0.355, accuracy = 1.000\n",
      "iteration (8200): loss = 0.163, accuracy = 1.000\n",
      "iteration (8250): loss = 0.071, accuracy = 1.000\n",
      "iteration (8300): loss = 0.146, accuracy = 1.000\n",
      "iteration (8350): loss = 0.981, accuracy = 0.992\n",
      "iteration (8400): loss = 1.913, accuracy = 0.984\n",
      "validation for epoch 17\n",
      "-  epoch 17: Train accuracy = 0.999\n",
      "-  epoch 17: Test accuracy = 0.985\n",
      "train for epoch 18\n",
      "iteration (8450): loss = 3.068, accuracy = 0.992\n",
      "iteration (8500): loss = 0.023, accuracy = 1.000\n",
      "iteration (8550): loss = 0.024, accuracy = 1.000\n",
      "iteration (8600): loss = 0.005, accuracy = 1.000\n",
      "iteration (8650): loss = 0.027, accuracy = 1.000\n",
      "iteration (8700): loss = 0.022, accuracy = 1.000\n",
      "iteration (8750): loss = 0.026, accuracy = 1.000\n",
      "iteration (8800): loss = 0.026, accuracy = 1.000\n",
      "iteration (8850): loss = 1.531, accuracy = 0.992\n",
      "validation for epoch 18\n",
      "-  epoch 18: Train accuracy = 0.999\n",
      "-  epoch 18: Test accuracy = 0.988\n",
      "train for epoch 19\n",
      "iteration (8900): loss = 0.261, accuracy = 1.000\n",
      "iteration (8950): loss = 0.680, accuracy = 1.000\n",
      "iteration (9000): loss = 0.003, accuracy = 1.000\n",
      "iteration (9050): loss = 0.051, accuracy = 1.000\n",
      "iteration (9100): loss = 0.032, accuracy = 1.000\n",
      "iteration (9150): loss = 0.001, accuracy = 1.000\n",
      "iteration (9200): loss = 0.002, accuracy = 1.000\n",
      "iteration (9250): loss = 0.085, accuracy = 1.000\n",
      "iteration (9300): loss = 0.605, accuracy = 1.000\n",
      "iteration (9350): loss = 0.005, accuracy = 1.000\n",
      "validation for epoch 19\n",
      "-  epoch 19: Train accuracy = 1.000\n",
      "-  epoch 19: Test accuracy = 0.988\n",
      "train for epoch 20\n",
      "iteration (9400): loss = 0.014, accuracy = 1.000\n",
      "iteration (9450): loss = 0.045, accuracy = 1.000\n",
      "iteration (9500): loss = 0.042, accuracy = 1.000\n",
      "iteration (9550): loss = 0.001, accuracy = 1.000\n",
      "iteration (9600): loss = 0.044, accuracy = 1.000\n",
      "iteration (9650): loss = 0.020, accuracy = 1.000\n",
      "iteration (9700): loss = 0.001, accuracy = 1.000\n",
      "iteration (9750): loss = 0.003, accuracy = 1.000\n",
      "iteration (9800): loss = 0.001, accuracy = 1.000\n",
      "validation for epoch 20\n",
      "-  epoch 20: Train accuracy = 1.000\n",
      "-  epoch 20: Test accuracy = 0.989\n",
      "train for epoch 21\n",
      "iteration (9850): loss = 0.012, accuracy = 1.000\n",
      "iteration (9900): loss = 0.012, accuracy = 1.000\n",
      "iteration (9950): loss = 0.000, accuracy = 1.000\n",
      "iteration (10000): loss = 0.001, accuracy = 1.000\n",
      "iteration (10050): loss = 0.007, accuracy = 1.000\n",
      "iteration (10100): loss = 0.009, accuracy = 1.000\n",
      "iteration (10150): loss = 0.023, accuracy = 1.000\n",
      "iteration (10200): loss = 0.000, accuracy = 1.000\n",
      "iteration (10250): loss = 0.004, accuracy = 1.000\n",
      "validation for epoch 21\n",
      "-  epoch 21: Train accuracy = 1.000\n",
      "-  epoch 21: Test accuracy = 0.989\n",
      "train for epoch 22\n",
      "iteration (10300): loss = 0.014, accuracy = 1.000\n",
      "iteration (10350): loss = 0.003, accuracy = 1.000\n",
      "iteration (10400): loss = 0.004, accuracy = 1.000\n",
      "iteration (10450): loss = 0.001, accuracy = 1.000\n",
      "iteration (10500): loss = 0.004, accuracy = 1.000\n",
      "iteration (10550): loss = 0.003, accuracy = 1.000\n",
      "iteration (10600): loss = 0.001, accuracy = 1.000\n",
      "iteration (10650): loss = 0.009, accuracy = 1.000\n",
      "iteration (10700): loss = 0.035, accuracy = 1.000\n",
      "iteration (10750): loss = 0.001, accuracy = 1.000\n",
      "validation for epoch 22\n",
      "-  epoch 22: Train accuracy = 1.000\n",
      "-  epoch 22: Test accuracy = 0.988\n",
      "train for epoch 23\n",
      "iteration (10800): loss = 0.001, accuracy = 1.000\n",
      "iteration (10850): loss = 0.006, accuracy = 1.000\n",
      "iteration (10900): loss = 0.002, accuracy = 1.000\n",
      "iteration (10950): loss = 0.004, accuracy = 1.000\n",
      "iteration (11000): loss = 0.001, accuracy = 1.000\n",
      "iteration (11050): loss = 0.001, accuracy = 1.000\n",
      "iteration (11100): loss = 0.007, accuracy = 1.000\n",
      "iteration (11150): loss = 0.003, accuracy = 1.000\n",
      "iteration (11200): loss = 0.001, accuracy = 1.000\n",
      "validation for epoch 23\n",
      "-  epoch 23: Train accuracy = 1.000\n",
      "-  epoch 23: Test accuracy = 0.988\n",
      "train for epoch 24\n",
      "iteration (11250): loss = 0.005, accuracy = 1.000\n",
      "iteration (11300): loss = 0.002, accuracy = 1.000\n",
      "iteration (11350): loss = 0.001, accuracy = 1.000\n",
      "iteration (11400): loss = 0.000, accuracy = 1.000\n",
      "iteration (11450): loss = 0.001, accuracy = 1.000\n",
      "iteration (11500): loss = 0.011, accuracy = 1.000\n",
      "iteration (11550): loss = 0.004, accuracy = 1.000\n",
      "iteration (11600): loss = 0.003, accuracy = 1.000\n",
      "iteration (11650): loss = 0.001, accuracy = 1.000\n",
      "validation for epoch 24\n",
      "-  epoch 24: Train accuracy = 1.000\n",
      "-  epoch 24: Test accuracy = 0.989\n",
      "train for epoch 25\n",
      "iteration (11700): loss = 0.001, accuracy = 1.000\n",
      "iteration (11750): loss = 0.005, accuracy = 1.000\n",
      "iteration (11800): loss = 0.014, accuracy = 1.000\n",
      "iteration (11850): loss = 0.001, accuracy = 1.000\n",
      "iteration (11900): loss = 0.037, accuracy = 1.000\n",
      "iteration (11950): loss = 0.002, accuracy = 1.000\n",
      "iteration (12000): loss = 0.012, accuracy = 1.000\n",
      "iteration (12050): loss = 0.003, accuracy = 1.000\n",
      "iteration (12100): loss = 0.004, accuracy = 1.000\n",
      "iteration (12150): loss = 0.002, accuracy = 1.000\n",
      "validation for epoch 25\n",
      "-  epoch 25: Train accuracy = 1.000\n",
      "-  epoch 25: Test accuracy = 0.989\n",
      "train for epoch 26\n",
      "iteration (12200): loss = 0.001, accuracy = 1.000\n",
      "iteration (12250): loss = 0.000, accuracy = 1.000\n",
      "iteration (12300): loss = 0.008, accuracy = 1.000\n",
      "iteration (12350): loss = 0.001, accuracy = 1.000\n",
      "iteration (12400): loss = 0.001, accuracy = 1.000\n",
      "iteration (12450): loss = 0.001, accuracy = 1.000\n",
      "iteration (12500): loss = 0.001, accuracy = 1.000\n",
      "iteration (12550): loss = 0.002, accuracy = 1.000\n",
      "iteration (12600): loss = 0.005, accuracy = 1.000\n",
      "validation for epoch 26\n",
      "-  epoch 26: Train accuracy = 1.000\n",
      "-  epoch 26: Test accuracy = 0.989\n",
      "train for epoch 27\n",
      "iteration (12650): loss = 0.000, accuracy = 1.000\n",
      "iteration (12700): loss = 0.003, accuracy = 1.000\n",
      "iteration (12750): loss = 0.001, accuracy = 1.000\n",
      "iteration (12800): loss = 0.000, accuracy = 1.000\n",
      "iteration (12850): loss = 0.003, accuracy = 1.000\n",
      "iteration (12900): loss = 0.001, accuracy = 1.000\n",
      "iteration (12950): loss = 0.002, accuracy = 1.000\n",
      "iteration (13000): loss = 0.001, accuracy = 1.000\n",
      "iteration (13050): loss = 0.000, accuracy = 1.000\n",
      "iteration (13100): loss = 0.000, accuracy = 1.000\n",
      "validation for epoch 27\n",
      "-  epoch 27: Train accuracy = 1.000\n",
      "-  epoch 27: Test accuracy = 0.989\n",
      "train for epoch 28\n",
      "iteration (13150): loss = 0.001, accuracy = 1.000\n",
      "iteration (13200): loss = 0.001, accuracy = 1.000\n",
      "iteration (13250): loss = 0.000, accuracy = 1.000\n",
      "iteration (13300): loss = 0.001, accuracy = 1.000\n",
      "iteration (13350): loss = 0.000, accuracy = 1.000\n",
      "iteration (13400): loss = 0.001, accuracy = 1.000\n",
      "iteration (13450): loss = 0.000, accuracy = 1.000\n",
      "iteration (13500): loss = 0.001, accuracy = 1.000\n",
      "iteration (13550): loss = 0.000, accuracy = 1.000\n",
      "validation for epoch 28\n",
      "-  epoch 28: Train accuracy = 1.000\n",
      "-  epoch 28: Test accuracy = 0.989\n",
      "train for epoch 29\n",
      "iteration (13600): loss = 0.000, accuracy = 1.000\n",
      "iteration (13650): loss = 0.002, accuracy = 1.000\n",
      "iteration (13700): loss = 0.000, accuracy = 1.000\n",
      "iteration (13750): loss = 0.000, accuracy = 1.000\n",
      "iteration (13800): loss = 0.002, accuracy = 1.000\n",
      "iteration (13850): loss = 0.002, accuracy = 1.000\n",
      "iteration (13900): loss = 0.000, accuracy = 1.000\n",
      "iteration (13950): loss = 0.000, accuracy = 1.000\n",
      "iteration (14000): loss = 0.000, accuracy = 1.000\n",
      "validation for epoch 29\n",
      "-  epoch 29: Train accuracy = 1.000\n",
      "-  epoch 29: Test accuracy = 0.989\n",
      "train for epoch 30\n",
      "iteration (14050): loss = 0.002, accuracy = 1.000\n",
      "iteration (14100): loss = 0.001, accuracy = 1.000\n",
      "iteration (14150): loss = 0.000, accuracy = 1.000\n",
      "iteration (14200): loss = 0.000, accuracy = 1.000\n",
      "iteration (14250): loss = 0.000, accuracy = 1.000\n",
      "iteration (14300): loss = 0.001, accuracy = 1.000\n",
      "iteration (14350): loss = 0.002, accuracy = 1.000\n",
      "iteration (14400): loss = 0.001, accuracy = 1.000\n",
      "iteration (14450): loss = 0.000, accuracy = 1.000\n",
      "iteration (14500): loss = 0.000, accuracy = 1.000\n",
      "validation for epoch 30\n",
      "-  epoch 30: Train accuracy = 1.000\n",
      "-  epoch 30: Test accuracy = 0.989\n",
      "train for epoch 31\n",
      "iteration (14550): loss = 0.000, accuracy = 1.000\n",
      "iteration (14600): loss = 0.001, accuracy = 1.000\n",
      "iteration (14650): loss = 0.001, accuracy = 1.000\n",
      "iteration (14700): loss = 0.001, accuracy = 1.000\n",
      "iteration (14750): loss = 0.000, accuracy = 1.000\n",
      "iteration (14800): loss = 0.000, accuracy = 1.000\n",
      "iteration (14850): loss = 0.000, accuracy = 1.000\n",
      "iteration (14900): loss = 0.001, accuracy = 1.000\n",
      "iteration (14950): loss = 0.000, accuracy = 1.000\n",
      "validation for epoch 31\n",
      "-  epoch 31: Train accuracy = 1.000\n",
      "-  epoch 31: Test accuracy = 0.990\n",
      "train for epoch 32\n",
      "iteration (15000): loss = 0.001, accuracy = 1.000\n",
      "iteration (15050): loss = 0.000, accuracy = 1.000\n",
      "iteration (15100): loss = 0.000, accuracy = 1.000\n",
      "iteration (15150): loss = 0.001, accuracy = 1.000\n",
      "iteration (15200): loss = 0.000, accuracy = 1.000\n",
      "iteration (15250): loss = 0.000, accuracy = 1.000\n",
      "iteration (15300): loss = 0.000, accuracy = 1.000\n",
      "iteration (15350): loss = 0.001, accuracy = 1.000\n",
      "iteration (15400): loss = 0.000, accuracy = 1.000\n",
      "validation for epoch 32\n",
      "-  epoch 32: Train accuracy = 1.000\n",
      "-  epoch 32: Test accuracy = 0.990\n",
      "train for epoch 33\n",
      "iteration (15450): loss = 0.001, accuracy = 1.000\n",
      "iteration (15500): loss = 0.001, accuracy = 1.000\n",
      "iteration (15550): loss = 0.000, accuracy = 1.000\n",
      "iteration (15600): loss = 0.000, accuracy = 1.000\n",
      "iteration (15650): loss = 0.001, accuracy = 1.000\n",
      "iteration (15700): loss = 0.001, accuracy = 1.000\n",
      "iteration (15750): loss = 0.001, accuracy = 1.000\n",
      "iteration (15800): loss = 0.000, accuracy = 1.000\n",
      "iteration (15850): loss = 0.000, accuracy = 1.000\n",
      "iteration (15900): loss = 0.000, accuracy = 1.000\n",
      "validation for epoch 33\n",
      "-  epoch 33: Train accuracy = 1.000\n",
      "-  epoch 33: Test accuracy = 0.990\n",
      "train for epoch 34\n",
      "iteration (15950): loss = 0.000, accuracy = 1.000\n",
      "iteration (16000): loss = 0.000, accuracy = 1.000\n",
      "iteration (16050): loss = 0.000, accuracy = 1.000\n",
      "iteration (16100): loss = 0.000, accuracy = 1.000\n",
      "iteration (16150): loss = 0.000, accuracy = 1.000\n",
      "iteration (16200): loss = 0.000, accuracy = 1.000\n",
      "iteration (16250): loss = 0.000, accuracy = 1.000\n",
      "iteration (16300): loss = 0.000, accuracy = 1.000\n",
      "iteration (16350): loss = 0.000, accuracy = 1.000\n",
      "validation for epoch 34\n",
      "-  epoch 34: Train accuracy = 1.000\n",
      "-  epoch 34: Test accuracy = 0.990\n",
      "train for epoch 35\n",
      "iteration (16400): loss = 0.000, accuracy = 1.000\n",
      "iteration (16450): loss = 0.000, accuracy = 1.000\n",
      "iteration (16500): loss = 0.000, accuracy = 1.000\n",
      "iteration (16550): loss = 0.000, accuracy = 1.000\n",
      "iteration (16600): loss = 0.000, accuracy = 1.000\n",
      "iteration (16650): loss = 0.000, accuracy = 1.000\n",
      "iteration (16700): loss = 0.000, accuracy = 1.000\n",
      "iteration (16750): loss = 0.000, accuracy = 1.000\n",
      "iteration (16800): loss = 0.000, accuracy = 1.000\n",
      "validation for epoch 35\n",
      "-  epoch 35: Train accuracy = 1.000\n",
      "-  epoch 35: Test accuracy = 0.990\n",
      "train for epoch 36\n",
      "iteration (16850): loss = 0.000, accuracy = 1.000\n",
      "iteration (16900): loss = 0.000, accuracy = 1.000\n",
      "iteration (16950): loss = 0.000, accuracy = 1.000\n",
      "iteration (17000): loss = 0.000, accuracy = 1.000\n",
      "iteration (17050): loss = 0.001, accuracy = 1.000\n",
      "iteration (17100): loss = 0.000, accuracy = 1.000\n",
      "iteration (17150): loss = 0.000, accuracy = 1.000\n",
      "iteration (17200): loss = 0.000, accuracy = 1.000\n",
      "iteration (17250): loss = 0.000, accuracy = 1.000\n",
      "iteration (17300): loss = 0.000, accuracy = 1.000\n",
      "validation for epoch 36\n",
      "-  epoch 36: Train accuracy = 1.000\n",
      "-  epoch 36: Test accuracy = 0.990\n",
      "train for epoch 37\n",
      "iteration (17350): loss = 0.001, accuracy = 1.000\n",
      "iteration (17400): loss = 0.000, accuracy = 1.000\n",
      "iteration (17450): loss = 0.000, accuracy = 1.000\n",
      "iteration (17500): loss = 0.000, accuracy = 1.000\n",
      "iteration (17550): loss = 0.000, accuracy = 1.000\n",
      "iteration (17600): loss = 0.000, accuracy = 1.000\n",
      "iteration (17650): loss = 0.000, accuracy = 1.000\n",
      "iteration (17700): loss = 0.000, accuracy = 1.000\n",
      "iteration (17750): loss = 0.000, accuracy = 1.000\n",
      "validation for epoch 37\n",
      "-  epoch 37: Train accuracy = 1.000\n",
      "-  epoch 37: Test accuracy = 0.990\n",
      "train for epoch 38\n",
      "iteration (17800): loss = 0.000, accuracy = 1.000\n",
      "iteration (17850): loss = 0.000, accuracy = 1.000\n",
      "iteration (17900): loss = 0.000, accuracy = 1.000\n",
      "iteration (17950): loss = 0.000, accuracy = 1.000\n",
      "iteration (18000): loss = 0.000, accuracy = 1.000\n",
      "iteration (18050): loss = 0.000, accuracy = 1.000\n",
      "iteration (18100): loss = 0.000, accuracy = 1.000\n",
      "iteration (18150): loss = 0.000, accuracy = 1.000\n",
      "iteration (18200): loss = 0.000, accuracy = 1.000\n",
      "iteration (18250): loss = 0.000, accuracy = 1.000\n",
      "validation for epoch 38\n",
      "-  epoch 38: Train accuracy = 1.000\n",
      "-  epoch 38: Test accuracy = 0.990\n",
      "train for epoch 39\n",
      "iteration (18300): loss = 0.000, accuracy = 1.000\n",
      "iteration (18350): loss = 0.000, accuracy = 1.000\n",
      "iteration (18400): loss = 0.000, accuracy = 1.000\n",
      "iteration (18450): loss = 0.000, accuracy = 1.000\n",
      "iteration (18500): loss = 0.000, accuracy = 1.000\n",
      "iteration (18550): loss = 0.000, accuracy = 1.000\n",
      "iteration (18600): loss = 0.000, accuracy = 1.000\n",
      "iteration (18650): loss = 0.000, accuracy = 1.000\n",
      "iteration (18700): loss = 0.000, accuracy = 1.000\n",
      "validation for epoch 39\n",
      "-  epoch 39: Train accuracy = 1.000\n",
      "-  epoch 39: Test accuracy = 0.990\n",
      "train for epoch 40\n",
      "iteration (18750): loss = 0.000, accuracy = 1.000\n",
      "iteration (18800): loss = 0.000, accuracy = 1.000\n",
      "iteration (18850): loss = 0.000, accuracy = 1.000\n",
      "iteration (18900): loss = 0.000, accuracy = 1.000\n",
      "iteration (18950): loss = 0.000, accuracy = 1.000\n",
      "iteration (19000): loss = 0.000, accuracy = 1.000\n",
      "iteration (19050): loss = 0.000, accuracy = 1.000\n",
      "iteration (19100): loss = 0.000, accuracy = 1.000\n",
      "iteration (19150): loss = 0.000, accuracy = 1.000\n",
      "validation for epoch 40\n",
      "-  epoch 40: Train accuracy = 1.000\n",
      "-  epoch 40: Test accuracy = 0.990\n",
      "train for epoch 41\n",
      "iteration (19200): loss = 0.000, accuracy = 1.000\n",
      "iteration (19250): loss = 0.000, accuracy = 1.000\n",
      "iteration (19300): loss = 0.000, accuracy = 1.000\n",
      "iteration (19350): loss = 0.000, accuracy = 1.000\n",
      "iteration (19400): loss = 0.000, accuracy = 1.000\n",
      "iteration (19450): loss = 0.000, accuracy = 1.000\n",
      "iteration (19500): loss = 0.000, accuracy = 1.000\n",
      "iteration (19550): loss = 0.000, accuracy = 1.000\n",
      "iteration (19600): loss = 0.000, accuracy = 1.000\n",
      "iteration (19650): loss = 0.000, accuracy = 1.000\n",
      "validation for epoch 41\n",
      "-  epoch 41: Train accuracy = 1.000\n",
      "-  epoch 41: Test accuracy = 0.990\n",
      "train for epoch 42\n",
      "iteration (19700): loss = 0.000, accuracy = 1.000\n",
      "iteration (19750): loss = 0.000, accuracy = 1.000\n",
      "iteration (19800): loss = 0.000, accuracy = 1.000\n",
      "iteration (19850): loss = 0.000, accuracy = 1.000\n",
      "iteration (19900): loss = 0.000, accuracy = 1.000\n",
      "iteration (19950): loss = 0.000, accuracy = 1.000\n",
      "iteration (20000): loss = 0.000, accuracy = 1.000\n",
      "iteration (20050): loss = 0.000, accuracy = 1.000\n",
      "iteration (20100): loss = 0.000, accuracy = 1.000\n",
      "validation for epoch 42\n",
      "-  epoch 42: Train accuracy = 1.000\n",
      "-  epoch 42: Test accuracy = 0.990\n",
      "train for epoch 43\n",
      "iteration (20150): loss = 0.000, accuracy = 1.000\n",
      "iteration (20200): loss = 0.000, accuracy = 1.000\n",
      "iteration (20250): loss = 0.000, accuracy = 1.000\n",
      "iteration (20300): loss = 0.000, accuracy = 1.000\n",
      "iteration (20350): loss = 0.000, accuracy = 1.000\n",
      "iteration (20400): loss = 0.000, accuracy = 1.000\n",
      "iteration (20450): loss = 0.000, accuracy = 1.000\n",
      "iteration (20500): loss = 0.000, accuracy = 1.000\n",
      "iteration (20550): loss = 0.000, accuracy = 1.000\n",
      "validation for epoch 43\n",
      "-  epoch 43: Train accuracy = 1.000\n",
      "-  epoch 43: Test accuracy = 0.990\n",
      "train for epoch 44\n",
      "iteration (20600): loss = 0.000, accuracy = 1.000\n",
      "iteration (20650): loss = 0.000, accuracy = 1.000\n",
      "iteration (20700): loss = 0.000, accuracy = 1.000\n",
      "iteration (20750): loss = 0.000, accuracy = 1.000\n",
      "iteration (20800): loss = 0.000, accuracy = 1.000\n",
      "iteration (20850): loss = 0.000, accuracy = 1.000\n",
      "iteration (20900): loss = 0.000, accuracy = 1.000\n",
      "iteration (20950): loss = 0.000, accuracy = 1.000\n",
      "iteration (21000): loss = 0.000, accuracy = 1.000\n",
      "iteration (21050): loss = 0.000, accuracy = 1.000\n",
      "validation for epoch 44\n",
      "-  epoch 44: Train accuracy = 1.000\n",
      "-  epoch 44: Test accuracy = 0.990\n",
      "train for epoch 45\n",
      "iteration (21100): loss = 0.000, accuracy = 1.000\n",
      "iteration (21150): loss = 0.000, accuracy = 1.000\n",
      "iteration (21200): loss = 0.000, accuracy = 1.000\n",
      "iteration (21250): loss = 0.000, accuracy = 1.000\n",
      "iteration (21300): loss = 0.000, accuracy = 1.000\n",
      "iteration (21350): loss = 0.000, accuracy = 1.000\n",
      "iteration (21400): loss = 0.000, accuracy = 1.000\n",
      "iteration (21450): loss = 0.000, accuracy = 1.000\n",
      "iteration (21500): loss = 0.000, accuracy = 1.000\n",
      "validation for epoch 45\n",
      "-  epoch 45: Train accuracy = 1.000\n",
      "-  epoch 45: Test accuracy = 0.990\n",
      "train for epoch 46\n",
      "iteration (21550): loss = 0.000, accuracy = 1.000\n",
      "iteration (21600): loss = 0.000, accuracy = 1.000\n",
      "iteration (21650): loss = 0.000, accuracy = 1.000\n",
      "iteration (21700): loss = 0.000, accuracy = 1.000\n",
      "iteration (21750): loss = 0.000, accuracy = 1.000\n",
      "iteration (21800): loss = 0.000, accuracy = 1.000\n",
      "iteration (21850): loss = 0.000, accuracy = 1.000\n",
      "iteration (21900): loss = 0.000, accuracy = 1.000\n",
      "iteration (21950): loss = 0.000, accuracy = 1.000\n",
      "validation for epoch 46\n",
      "-  epoch 46: Train accuracy = 1.000\n",
      "-  epoch 46: Test accuracy = 0.990\n",
      "train for epoch 47\n",
      "iteration (22000): loss = 0.000, accuracy = 1.000\n",
      "iteration (22050): loss = 0.000, accuracy = 1.000\n",
      "iteration (22100): loss = 0.000, accuracy = 1.000\n",
      "iteration (22150): loss = 0.000, accuracy = 1.000\n",
      "iteration (22200): loss = 0.000, accuracy = 1.000\n",
      "iteration (22250): loss = 0.000, accuracy = 1.000\n",
      "iteration (22300): loss = 0.000, accuracy = 1.000\n",
      "iteration (22350): loss = 0.000, accuracy = 1.000\n",
      "iteration (22400): loss = 0.000, accuracy = 1.000\n",
      "iteration (22450): loss = 0.000, accuracy = 1.000\n",
      "validation for epoch 47\n",
      "-  epoch 47: Train accuracy = 1.000\n",
      "-  epoch 47: Test accuracy = 0.990\n",
      "train for epoch 48\n",
      "iteration (22500): loss = 0.000, accuracy = 1.000\n",
      "iteration (22550): loss = 0.000, accuracy = 1.000\n",
      "iteration (22600): loss = 0.000, accuracy = 1.000\n",
      "iteration (22650): loss = 0.000, accuracy = 1.000\n",
      "iteration (22700): loss = 0.000, accuracy = 1.000\n",
      "iteration (22750): loss = 0.000, accuracy = 1.000\n",
      "iteration (22800): loss = 0.000, accuracy = 1.000\n",
      "iteration (22850): loss = 0.000, accuracy = 1.000\n",
      "iteration (22900): loss = 0.000, accuracy = 1.000\n",
      "validation for epoch 48\n",
      "-  epoch 48: Train accuracy = 1.000\n",
      "-  epoch 48: Test accuracy = 0.990\n",
      "train for epoch 49\n",
      "iteration (22950): loss = 0.000, accuracy = 1.000\n",
      "iteration (23000): loss = 0.000, accuracy = 1.000\n",
      "iteration (23050): loss = 0.000, accuracy = 1.000\n",
      "iteration (23100): loss = 0.000, accuracy = 1.000\n",
      "iteration (23150): loss = 0.000, accuracy = 1.000\n",
      "iteration (23200): loss = 0.000, accuracy = 1.000\n",
      "iteration (23250): loss = 0.000, accuracy = 1.000\n",
      "iteration (23300): loss = 0.000, accuracy = 1.000\n",
      "iteration (23350): loss = 0.000, accuracy = 1.000\n",
      "validation for epoch 49\n",
      "-  epoch 49: Train accuracy = 1.000\n",
      "-  epoch 49: Test accuracy = 0.990\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAZIAAAEKCAYAAAA4t9PUAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nIul9roE8xV0AAAAASUVORK5CYII=\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x1b1b2465ba8>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAZIAAAEKCAYAAAA4t9PUAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n/wcmxoBdalvHNwAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x1b3663af908>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Highest test accuracy is: 0.990384615385\n",
      "***** Last accuracy: 0.990\n"
     ]
    }
   ],
   "source": [
    "tf.reset_default_graph()\n",
    "with tf.Session() as sess:\n",
    "    model = Super_Le_Net5()\n",
    "    model.train(sess, X_train, Y_train, X_test, Y_test)\n",
    "    accuracy = model.evaluate(sess, X_test, Y_test)\n",
    "    print('***** Last accuracy: %.3f' % accuracy)\n",
    "        \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [conda root]",
   "language": "python",
   "name": "conda-root-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
