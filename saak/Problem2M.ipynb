{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load libs\n",
    "import torch\n",
    "import argparse\n",
    "from torchvision import datasets, transforms\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from data.datasets import MNIST\n",
    "import torch.utils.data as data_utils\n",
    "from sklearn.decomposition import PCA\n",
    "import torch.nn.functional as F\n",
    "from torch.autograd import Variable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.2.0_3\n"
     ]
    }
   ],
   "source": [
    "print (torch.__version__)\n",
    "batch_size=1\n",
    "test_batch_size=1\n",
    "kwargs={}\n",
    "train_loader=data_utils.DataLoader(MNIST(root='./data',train=True,process=False,transform=transforms.Compose([\n",
    "    transforms.Scale((32,32)),\n",
    "    transforms.ToTensor(),\n",
    "])),batch_size=batch_size,shuffle=True,**kwargs)\n",
    "\n",
    "\n",
    "test_loader=data_utils.DataLoader(MNIST(root='./data',train=False,process=False,transform=transforms.Compose([\n",
    "    transforms.Scale((32,32)),\n",
    "    transforms.ToTensor(),\n",
    "])),batch_size=test_batch_size,shuffle=True,**kwargs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_all_train_dataset():\n",
    "    datasets = []\n",
    "    train_label = []\n",
    "    for data in train_loader:\n",
    "        data_numpy = data[0].numpy()\n",
    "        label_numpy = data[1].numpy()\n",
    "        data_numpy = np.squeeze(data_numpy)\n",
    "        datasets.append(data_numpy)\n",
    "        train_label.append(label_numpy)\n",
    "\n",
    "    datasets = np.array(datasets)\n",
    "    datasets=np.expand_dims(datasets,axis=1)\n",
    "    print ('Numpy train dataset shape is {}'.format(datasets.shape))\n",
    "    return datasets,train_label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_all_test_dataset():\n",
    "    datasets = []\n",
    "    test_label = []\n",
    "    for data in test_loader:\n",
    "        data_numpy = data[0].numpy()\n",
    "        label_numpy = data[1].numpy()\n",
    "        data_numpy = np.squeeze(data_numpy)\n",
    "        datasets.append(data_numpy)\n",
    "        test_label.append(label_numpy)\n",
    "\n",
    "    datasets = np.array(datasets)\n",
    "    datasets=np.expand_dims(datasets,axis=1)\n",
    "    print ('Numpy test dataset shape is {}'.format(datasets.shape))\n",
    "    return datasets,test_label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def PCA_and_augment(data_in, num_key_comp):\n",
    "    # data reshape\n",
    "    data=np.reshape(data_in,(data_in.shape[0],-1))\n",
    "    print ('PCA_and_augment: {}'.format(data.shape))\n",
    "    # mean removal\n",
    "    mean = np.mean(data, axis=0)\n",
    "    datas_mean_remov = data - mean\n",
    "    print ('PCA_and_augment meanremove shape: {}'.format(datas_mean_remov.shape))\n",
    "\n",
    "    # PCA, retain all components\n",
    "    #pca=PCA(n_components = num_key_comp)\n",
    "    pca=PCA(n_components=num_key_comp)\n",
    "    pca.fit(datas_mean_remov)\n",
    "    \n",
    "    #eng=np.cumsum(pca.explained_variance_ratio_)\n",
    "    #f_num = np.count_nonzero(eng < 0.999)\n",
    "    #comps=pca.components_[:f_num,:]\n",
    "    comps=pca.components_\n",
    "    \n",
    "    # augment, DC component doesn't\n",
    "    comps_aug=[vec*(-1) for vec in comps[:-1]]\n",
    "    comps_complete=np.vstack((comps,comps_aug))\n",
    "    print ('PCA_and_augment comps_complete shape: {}'.format(comps_complete.shape))\n",
    "    return comps_complete"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from itertools import product\n",
    "def fit_pca_shape(datasets,depth):\n",
    "    factor=np.power(2,depth)\n",
    "    length=32/factor\n",
    "    print ('fit_pca_shape: length: {}'.format(length))\n",
    "    idx1=range(0,int(length),2)\n",
    "    idx2=[i+2 for i in idx1]\n",
    "    print ('fit_pca_shape: idx1: {}'.format(idx1))\n",
    "    data_lattice=[datasets[:,:,i:j,k:l] for ((i,j),(k,l)) in product(zip(idx1,idx2),zip(idx1,idx2))]\n",
    "    data_lattice=np.array(data_lattice)\n",
    "    print ('fit_pca_shape: data_lattice.shape: {}'.format(data_lattice.shape))\n",
    "\n",
    "    #shape reshape\n",
    "    data=np.reshape(data_lattice,(data_lattice.shape[0]*data_lattice.shape[1],data_lattice.shape[2],2,2))\n",
    "    print ('fit_pca_shape: reshape: {}'.format(data.shape))\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ret_filt_patches(aug_anchors,input_channels):\n",
    "    shape=int(aug_anchors.shape[1]/4)\n",
    "    num=int(aug_anchors.shape[0])\n",
    "    filt=np.reshape(aug_anchors,(num,shape,4))\n",
    "    \n",
    "    # reshape to kernels, (# output_channels,# input_channels,2,2)\n",
    "    filters=np.reshape(filt,(num,shape,2,2))\n",
    "\n",
    "    return filters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def conv_and_relu(filters,datasets,stride=2):\n",
    "    # torch data change\n",
    "    filters_t=torch.from_numpy(filters)\n",
    "    datasets_t=torch.from_numpy(datasets)\n",
    "\n",
    "    # Variables\n",
    "    filt=Variable(filters_t).type(torch.FloatTensor)\n",
    "    data=Variable(datasets_t).type(torch.FloatTensor)\n",
    "\n",
    "    # Convolution\n",
    "    output=F.conv2d(data,filt,stride=stride)\n",
    "\n",
    "    # Relu\n",
    "    relu_output=F.relu(output)\n",
    "\n",
    "    return relu_output,filt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def one_stage_saak_trans(datasets=None,depth=0,num_key_comp=5):\n",
    "\n",
    "    # intial dataset, (60000,1,32,32)\n",
    "    # channel change: 1->7\n",
    "    print ('one_stage_saak_trans: datasets.shape {}'.format(datasets.shape))\n",
    "    input_channels=datasets.shape[1]\n",
    "\n",
    "    # change data shape, (14*60000,4)\n",
    "    data_flatten=fit_pca_shape(datasets,depth)\n",
    "    \n",
    "    # augmented components, first round: (7,4), only augment AC components\n",
    "    comps_complete=PCA_and_augment(data_flatten,num_key_comp)\n",
    "    print ('one_stage_saak_trans: comps_complete: {}'.format(comps_complete.shape))\n",
    "\n",
    "    # get filter, (7,1,2,2) \n",
    "    filters=ret_filt_patches(comps_complete,input_channels)\n",
    "    print ('one_stage_saak_trans: filters: {}'.format(filters.shape))\n",
    "\n",
    "    # output (60000,7,14,14)\n",
    "    relu_output,filt=conv_and_relu(filters,datasets,stride=2)\n",
    "\n",
    "    data=relu_output.data.numpy()\n",
    "    print ('one_stage_saak_trans: output: {}'.format(data.shape))\n",
    "    return data,filt,relu_output,filters\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def five_stage_saak_trans():\n",
    "    filters = []\n",
    "   \n",
    "    data_train,train_label=create_all_train_dataset()\n",
    "    data_test,test_label = create_all_test_dataset()\n",
    "    original_train_dataset=data_train\n",
    "    original_test_dataset=data_test\n",
    "    \n",
    "    num_key_comp = [3,4,7,6,8]\n",
    "    \n",
    "    for i in range(5):\n",
    "        print ('{} stage of saak transform_train: '.format(i))      \n",
    "        data_train,filt,output,f=one_stage_saak_trans(data_train,depth=i,num_key_comp=num_key_comp[i])\n",
    "        filters.append(f)\n",
    "\n",
    "    for i in range(5):\n",
    "        print ('{} stage of saak transform_test: '.format(i))\n",
    "        relu_output,filt=conv_and_relu(filters[i],data_test,stride=2)\n",
    "        data_test=relu_output.data.numpy()\n",
    "        \n",
    "    return data_train,data_test,train_label,test_label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Numpy train dataset shape is (60000, 1, 32, 32)\n",
      "Numpy test dataset shape is (10000, 1, 32, 32)\n",
      "0 stage of saak transform_train: \n",
      "one_stage_saak_trans: datasets.shape (60000, 1, 32, 32)\n",
      "fit_pca_shape: length: 32.0\n",
      "fit_pca_shape: idx1: range(0, 32, 2)\n",
      "fit_pca_shape: data_lattice.shape: (256, 60000, 1, 2, 2)\n",
      "fit_pca_shape: reshape: (15360000, 1, 2, 2)\n",
      "PCA_and_augment: (15360000, 4)\n",
      "PCA_and_augment meanremove shape: (15360000, 4)\n",
      "PCA_and_augment comps_complete shape: (5, 4)\n",
      "one_stage_saak_trans: comps_complete: (5, 4)\n",
      "one_stage_saak_trans: filters: (5, 1, 2, 2)\n",
      "one_stage_saak_trans: output: (60000, 5, 16, 16)\n",
      "1 stage of saak transform_train: \n",
      "one_stage_saak_trans: datasets.shape (60000, 5, 16, 16)\n",
      "fit_pca_shape: length: 16.0\n",
      "fit_pca_shape: idx1: range(0, 16, 2)\n",
      "fit_pca_shape: data_lattice.shape: (64, 60000, 5, 2, 2)\n",
      "fit_pca_shape: reshape: (3840000, 5, 2, 2)\n",
      "PCA_and_augment: (3840000, 20)\n",
      "PCA_and_augment meanremove shape: (3840000, 20)\n",
      "PCA_and_augment comps_complete shape: (7, 20)\n",
      "one_stage_saak_trans: comps_complete: (7, 20)\n",
      "one_stage_saak_trans: filters: (7, 5, 2, 2)\n",
      "one_stage_saak_trans: output: (60000, 7, 8, 8)\n",
      "2 stage of saak transform_train: \n",
      "one_stage_saak_trans: datasets.shape (60000, 7, 8, 8)\n",
      "fit_pca_shape: length: 8.0\n",
      "fit_pca_shape: idx1: range(0, 8, 2)\n",
      "fit_pca_shape: data_lattice.shape: (16, 60000, 7, 2, 2)\n",
      "fit_pca_shape: reshape: (960000, 7, 2, 2)\n",
      "PCA_and_augment: (960000, 28)\n",
      "PCA_and_augment meanremove shape: (960000, 28)\n",
      "PCA_and_augment comps_complete shape: (13, 28)\n",
      "one_stage_saak_trans: comps_complete: (13, 28)\n",
      "one_stage_saak_trans: filters: (13, 7, 2, 2)\n",
      "one_stage_saak_trans: output: (60000, 13, 4, 4)\n",
      "3 stage of saak transform_train: \n",
      "one_stage_saak_trans: datasets.shape (60000, 13, 4, 4)\n",
      "fit_pca_shape: length: 4.0\n",
      "fit_pca_shape: idx1: range(0, 4, 2)\n",
      "fit_pca_shape: data_lattice.shape: (4, 60000, 13, 2, 2)\n",
      "fit_pca_shape: reshape: (240000, 13, 2, 2)\n",
      "PCA_and_augment: (240000, 52)\n",
      "PCA_and_augment meanremove shape: (240000, 52)\n",
      "PCA_and_augment comps_complete shape: (11, 52)\n",
      "one_stage_saak_trans: comps_complete: (11, 52)\n",
      "one_stage_saak_trans: filters: (11, 13, 2, 2)\n",
      "one_stage_saak_trans: output: (60000, 11, 2, 2)\n",
      "4 stage of saak transform_train: \n",
      "one_stage_saak_trans: datasets.shape (60000, 11, 2, 2)\n",
      "fit_pca_shape: length: 2.0\n",
      "fit_pca_shape: idx1: range(0, 2, 2)\n",
      "fit_pca_shape: data_lattice.shape: (1, 60000, 11, 2, 2)\n",
      "fit_pca_shape: reshape: (60000, 11, 2, 2)\n",
      "PCA_and_augment: (60000, 44)\n",
      "PCA_and_augment meanremove shape: (60000, 44)\n",
      "PCA_and_augment comps_complete shape: (15, 44)\n",
      "one_stage_saak_trans: comps_complete: (15, 44)\n",
      "one_stage_saak_trans: filters: (15, 11, 2, 2)\n",
      "one_stage_saak_trans: output: (60000, 15, 1, 1)\n",
      "0 stage of saak transform_test: \n",
      "1 stage of saak transform_test: \n",
      "2 stage of saak transform_test: \n",
      "3 stage of saak transform_test: \n",
      "4 stage of saak transform_test: \n"
     ]
    }
   ],
   "source": [
    "saak_train,saak_test,train_label,test_label=five_stage_saak_trans()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(60000, 15, 1, 1)\n",
      "(10000, 15, 1, 1)\n",
      "(60000, 15)\n",
      "(10000, 15)\n"
     ]
    }
   ],
   "source": [
    "print(saak_train.shape)\n",
    "print(saak_test.shape)\n",
    "#print(saak_train[0])\n",
    "saak_train = saak_train.reshape((60000,-1))\n",
    "saak_test = saak_test.reshape((10000,-1))\n",
    "print(saak_train.shape)\n",
    "print(saak_test.shape)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "#train_pca = PCA()\n",
    "#train_pca.fit(saak_train)\n",
    "#eng=np.cumsum(train_pca.explained_variance_ratio_)\n",
    "#f_num = np.count_nonzero(eng < 0.90)\n",
    "#print(f_num)\n",
    "#saak_train=train_pca.transform(saak_train)[:,:f_num]\n",
    "#saak_test=train_pca.transform(saak_test)[:,:f_num]\n",
    "#print(saak_train.shape)\n",
    "#print(saak_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def load_train_label():\n",
    "#     f = open('./data/raw/train-labels-idx1-ubyte')\n",
    "#     loaded = np.fromfile(file=f,dtype = np.uint8)\n",
    "#     loaded = loaded[8:].reshape(60000).astype(np.uint8)\n",
    "#     return loaded\n",
    "\n",
    "# train_label = load_train_label()\n",
    "# print(train_label.shape)\n",
    "# #print(train_label[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def load_test_label():\n",
    "#     f = open('./data/raw/t10k-labels-idx1-ubyte')\n",
    "#     loaded = np.fromfile(file=f,dtype = np.uint8)\n",
    "#     loaded = loaded[8:].reshape(10000).astype(np.uint8)\n",
    "#     return loaded\n",
    "# test_label = load_test_label()\n",
    "# print(test_label.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.svm import SVC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "svm_train = SVC(\n",
    "          C=1.0,\n",
    "          cache_size=200,\n",
    "          class_weight=None,\n",
    "          coef0=0.0,\n",
    "          decision_function_shape='ovr',\n",
    "          degree=3,\n",
    "          gamma='auto', \n",
    "          kernel='rbf',\n",
    "          max_iter=2000,\n",
    "          probability=False, \n",
    "          random_state=None, \n",
    "          shrinking=True,\n",
    "          tol=0.001,\n",
    "          verbose=False,\n",
    "             )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/tian/.local/lib/python3.5/site-packages/sklearn/utils/validation.py:578: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n",
      "/home/tian/.local/lib/python3.5/site-packages/sklearn/svm/base.py:218: ConvergenceWarning: Solver terminated early (max_iter=2000).  Consider pre-processing your data with StandardScaler or MinMaxScaler.\n",
      "  % self.max_iter, ConvergenceWarning)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "SVC(C=1.0, cache_size=200, class_weight=None, coef0=0.0,\n",
       "  decision_function_shape='ovr', degree=3, gamma='auto', kernel='rbf',\n",
       "  max_iter=2000, probability=False, random_state=None, shrinking=True,\n",
       "  tol=0.001, verbose=False)"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "svm_train.fit(saak_train,train_label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_result = svm_train.predict(saak_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_result = svm_train.predict(saak_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "#accuray_train=np.count_nonzero(train_result==train_label)\n",
    "#accuray_test=np.count_nonzero(test_result==test_label)\n",
    "#print(\"train_accuray is: \" + str(accuray_train/60000.0))\n",
    "#print(\"test_accuray is: \" + str(accuray_test/10000.0))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "num of correct classification_train: 50981\n",
      "accuray_train: 0.8496833333333333\n",
      "num of correct classification_test: 8464\n",
      "accuray_test: 0.8464\n"
     ]
    }
   ],
   "source": [
    "accuray = 0\n",
    "for i in range(60000):\n",
    "    if train_label[i]==train_result[i]:\n",
    "        accuray = accuray + 1\n",
    "print(\"num of correct classification_train: \" + str(accuray))\n",
    "print(\"accuray_train: \" + str(accuray/60000.0))\n",
    "accuray = 0\n",
    "for i in range(10000):\n",
    "    if test_label[i]==test_result[i]:\n",
    "        accuray = accuray + 1\n",
    "print(\"num of correct classification_test: \" + str(accuray))\n",
    "print(\"accuray_test: \" + str(accuray/10000.0))\n",
    "\n",
    "    \n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
